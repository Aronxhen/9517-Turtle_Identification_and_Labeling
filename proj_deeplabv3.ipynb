{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Deeplabv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.Create data dictionary by train, valid and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T18:48:54.267649Z",
     "start_time": "2024-11-02T18:48:53.816938Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:40.322833Z",
     "iopub.status.busy": "2024-11-03T11:21:40.322178Z",
     "iopub.status.idle": "2024-11-03T11:21:40.969751Z",
     "shell.execute_reply": "2024-11-03T11:21:40.968278Z",
     "shell.execute_reply.started": "2024-11-03T11:21:40.322805Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T18:48:55.361228Z",
     "start_time": "2024-11-02T18:48:55.239928Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:40.971469Z",
     "iopub.status.busy": "2024-11-03T11:21:40.971206Z",
     "iopub.status.idle": "2024-11-03T11:21:41.383407Z",
     "shell.execute_reply": "2024-11-03T11:21:41.382717Z",
     "shell.execute_reply.started": "2024-11-03T11:21:40.971445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set number: 5303\n",
      "valid set number: 1118\n",
      "test set number: 2308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>split_open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>images/t001/CAluWEgwPX.JPG</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>images/t001/EKyrFKHQzh.JPG</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>images/t001/ELAvEqeXxT.JPG</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>images/t001/IxRLFwTGCv.JPG</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>images/t001/LKCJAhfLBJ.JPG</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8724</th>\n",
       "      <td>8725</td>\n",
       "      <td>images/t610/miUGGSioXO.jpeg</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8725</th>\n",
       "      <td>8726</td>\n",
       "      <td>images/t610/aOzTdMCkzF.jpeg</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726</th>\n",
       "      <td>8727</td>\n",
       "      <td>images/t610/ZmTLXySHIZ.jpeg</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>8728</td>\n",
       "      <td>images/t610/qVDYBLbzda.jpeg</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>8729</td>\n",
       "      <td>images/t610/THVsdHexWD.jpeg</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8729 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                    file_name split_open\n",
       "0        1   images/t001/CAluWEgwPX.JPG      train\n",
       "1        2   images/t001/EKyrFKHQzh.JPG      train\n",
       "2        3   images/t001/ELAvEqeXxT.JPG      train\n",
       "3        4   images/t001/IxRLFwTGCv.JPG      train\n",
       "4        5   images/t001/LKCJAhfLBJ.JPG      train\n",
       "...    ...                          ...        ...\n",
       "8724  8725  images/t610/miUGGSioXO.jpeg       test\n",
       "8725  8726  images/t610/aOzTdMCkzF.jpeg       test\n",
       "8726  8727  images/t610/ZmTLXySHIZ.jpeg       test\n",
       "8727  8728  images/t610/qVDYBLbzda.jpeg       test\n",
       "8728  8729  images/t610/THVsdHexWD.jpeg       test\n",
       "\n",
       "[8729 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divide data from test valid train\n",
    "meta_split = pd.read_csv(f'/root/autodl-tmp/deeplabv3/archive/turtles-data/data/metadata_splits.csv')\n",
    "# get main data from csv\n",
    "meta_data = meta_split[['id', 'file_name', 'split_open']]\n",
    "# get types name\n",
    "data_type = set(meta_data['split_open'])\n",
    "\n",
    "# get each type set and their id and file names\n",
    "meta_data_dict = {}\n",
    "for type in data_type:\n",
    "    meta_data_dict[type] = []\n",
    "for index, row in meta_data.iterrows():\n",
    "    meta_data_dict[row['split_open']].append((row['id'], row['file_name']))\n",
    "\n",
    "print(f\"train set number: {len(meta_data_dict['train'])}\")\n",
    "print(f\"valid set number: {len(meta_data_dict['valid'])}\")\n",
    "print(f\"test set number: {len(meta_data_dict['test'])}\")\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Set class dataset by using coco\n",
    "\n",
    "for getting image and mask, then we can put into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T19:21:38.797792Z",
     "start_time": "2024-11-02T19:21:38.794131Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:41.384407Z",
     "iopub.status.busy": "2024-11-03T11:21:41.384176Z",
     "iopub.status.idle": "2024-11-03T11:21:46.532352Z",
     "shell.execute_reply": "2024-11-03T11:21:46.531464Z",
     "shell.execute_reply.started": "2024-11-03T11:21:41.384385Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.models.segmentation\n",
    "import torch.optim as optim \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T19:21:43.793679Z",
     "start_time": "2024-11-02T19:21:42.080450Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:46.533956Z",
     "iopub.status.busy": "2024-11-03T11:21:46.533435Z",
     "iopub.status.idle": "2024-11-03T11:21:51.147120Z",
     "shell.execute_reply": "2024-11-03T11:21:51.145924Z",
     "shell.execute_reply.started": "2024-11-03T11:21:46.533928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.17s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# set img dir\n",
    "image_dir = '/root/autodl-tmp/deeplabv3/archive/turtles-data/data/'       \n",
    "\n",
    "# json to coco\n",
    "coco = COCO('/root/autodl-tmp/deeplabv3/archive/turtles-data/data/annotations.json')\n",
    "\n",
    "# image and mask transform method\n",
    "transform_method = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "target_transform_method = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=Image.NEAREST), \n",
    "    transforms.Lambda(lambda mask: torch.from_numpy(np.array(mask)).long())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T19:22:52.853217Z",
     "start_time": "2024-11-02T19:22:52.847273Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:51.149082Z",
     "iopub.status.busy": "2024-11-03T11:21:51.148800Z",
     "iopub.status.idle": "2024-11-03T11:21:51.169600Z",
     "shell.execute_reply": "2024-11-03T11:21:51.168958Z",
     "shell.execute_reply.started": "2024-11-03T11:21:51.149056Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, coco, meta_data_dict, data_type, transform=None, target_transform=None):\n",
    "        self.image_dir = image_dir  # original image dir\n",
    "        self.coco = coco # coco\n",
    "        self.data_type = data_type # train valid test\n",
    "        self.transform = transform  # transform method\n",
    "        self.target_transform = target_transform  # transform method\n",
    "        self.images = [] # all original images path in train set\n",
    "\n",
    "        data_list = meta_data_dict[self.data_type]\n",
    "        img_Ids = self.coco.getImgIds()\n",
    "\n",
    "        for i in img_Ids:\n",
    "            # if img_id in data_type list is true\n",
    "            if any(t[0] == i for t in data_list):\n",
    "                self.images.append(self.coco.loadImgs(i)[0])\n",
    "\n",
    "    def process_mask(self, image_id, image):\n",
    "        category_map = {\n",
    "            1: 1,  # turtles\n",
    "            2: 2,  # flipper\n",
    "            3: 3   # head\n",
    "        }\n",
    "        \n",
    "        mask = np.zeros((image.size[1], image.size[0]), dtype=np.uint8)\n",
    "        for cat_id, target_value in category_map.items():\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id, catIds=cat_id, iscrowd=None)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                ann_mask = self.coco.annToMask(ann)\n",
    "                mask[ann_mask > 0] = target_value  \n",
    "                \n",
    "        return mask\n",
    "\n",
    "    def __len__(self):  # set dataset size\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        # get img id\n",
    "        img_id = img['id']\n",
    "        # get img path\n",
    "        img_file_name = img['file_name']\n",
    "\n",
    "        image = Image.open(f\"{self.image_dir}/\"+img_file_name).convert(\"RGB\") # transfor to PIL\n",
    "        mask = self.process_mask(img_id, image)\n",
    "        mask = Image.fromarray(mask.astype('uint8'))  # transfor to PIL\n",
    "\n",
    "        # transform image and mask to [C, H, W]\n",
    "        image = self.transform(image) \n",
    "        mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask \n",
    "    \n",
    "    # image: PIL [H,W,C] -> [C,H,W]\n",
    "    # mask: numpy -> PIL [H,W,C] -> [C,H,W]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Create dataset for train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T19:22:55.872279Z",
     "start_time": "2024-11-02T19:22:54.284244Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:51.172696Z",
     "iopub.status.busy": "2024-11-03T11:21:51.172432Z",
     "iopub.status.idle": "2024-11-03T11:21:53.263730Z",
     "shell.execute_reply": "2024-11-03T11:21:53.262775Z",
     "shell.execute_reply.started": "2024-11-03T11:21:51.172672Z"
    }
   },
   "outputs": [],
   "source": [
    "# create train dateset\n",
    "train_dataset = CustomDataset(\n",
    "    image_dir=image_dir,\n",
    "    coco=coco,\n",
    "    transform=transform_method,\n",
    "    meta_data_dict=meta_data_dict,\n",
    "    target_transform=target_transform_method,\n",
    "    data_type='train'\n",
    ")\n",
    "\n",
    "# create valid dataset\n",
    "valid_dataset = CustomDataset(\n",
    "    image_dir=image_dir,\n",
    "    coco=coco,\n",
    "    transform=transform_method,\n",
    "    meta_data_dict=meta_data_dict,\n",
    "    target_transform=target_transform_method,\n",
    "    data_type='valid'\n",
    ")\n",
    "\n",
    "# create test dataset\n",
    "test_dataset = CustomDataset(\n",
    "    image_dir=image_dir,\n",
    "    coco=coco,\n",
    "    transform=transform_method,\n",
    "    meta_data_dict=meta_data_dict,\n",
    "    target_transform=target_transform_method,\n",
    "    data_type='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Create dataloader\n",
    "divide dataset into samll dataset in order to improving effiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T19:22:58.090471Z",
     "start_time": "2024-11-02T19:22:57.448518Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:53.264983Z",
     "iopub.status.busy": "2024-11-03T11:21:53.264721Z",
     "iopub.status.idle": "2024-11-03T11:21:54.357878Z",
     "shell.execute_reply": "2024-11-03T11:21:54.357102Z",
     "shell.execute_reply.started": "2024-11-03T11:21:53.264957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images: torch.Size([16, 3, 512, 512])\n",
      "Batch of masks: torch.Size([16, 512, 512])\n",
      "Masks dtype: torch.int64\n",
      "Masks value: tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# create valid DataLoader\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=True)\n",
    "# create test DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "# create train DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# see the data set size\n",
    "for images, masks in train_dataloader:\n",
    "    print(\"Batch of images:\", images.shape)  # show the first image tensor\n",
    "    print(\"Batch of masks:\", masks.shape)    # show the first mask tensor\n",
    "    unique_values = torch.unique(masks)     \n",
    "    print(\"Masks dtype:\", masks.dtype)      # show the first mask dtype\n",
    "    print(\"Masks value:\", unique_values)    # show the first mask uniqe\n",
    "    break  # just want to the first one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Show dataloader size and dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T19:23:00.509639Z",
     "start_time": "2024-11-02T19:23:00.506945Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:54.359317Z",
     "iopub.status.busy": "2024-11-03T11:21:54.359012Z",
     "iopub.status.idle": "2024-11-03T11:21:54.364400Z",
     "shell.execute_reply": "2024-11-03T11:21:54.363733Z",
     "shell.execute_reply.started": "2024-11-03T11:21:54.359290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in the dataset: 5303\n",
      "Total number of images in the dataset: 1118\n",
      "Total number of images in the dataset: 2308\n",
      "Total number of images in the dataset: 332\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of images in the dataset:\", len(train_dataset))\n",
    "print(\"Total number of images in the dataset:\", len(valid_dataset))\n",
    "print(\"Total number of images in the dataset:\", len(test_dataset))\n",
    "print(\"Total number of images in the dataset:\", len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Using deeplabv3 model to get iou and miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T19:01:59.998647Z",
     "start_time": "2024-11-02T19:01:59.631485Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:54.365483Z",
     "iopub.status.busy": "2024-11-03T11:21:54.365242Z",
     "iopub.status.idle": "2024-11-03T11:21:55.711107Z",
     "shell.execute_reply": "2024-11-03T11:21:55.709808Z",
     "shell.execute_reply.started": "2024-11-03T11:21:54.365460Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4  # set classes number\n",
    "\n",
    "# using deeplabv3\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False)\n",
    "\n",
    "# Modify the category header of the model to fit the number of custom categories\n",
    "model.classifier[4] = torch.nn.Conv2d(256, num_classes, kernel_size=(1,1))\n",
    "\n",
    "# using cuba gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer (Adam optimizer, learning rate 0.0001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T11:21:55.712489Z",
     "iopub.status.busy": "2024-11-03T11:21:55.712198Z",
     "iopub.status.idle": "2024-11-03T14:50:14.780959Z",
     "shell.execute_reply": "2024-11-03T14:50:14.777473Z",
     "shell.execute_reply.started": "2024-11-03T11:21:55.712463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Training Loss: 0.1506\n",
      "Epoch [1/25], Validation the average of three class miou: 0.7814\n",
      " - Class 1 'turtle' mIoU: 0.8714\n",
      " - Class 2 'flipper' mIoU: 0.7221\n",
      " - Class 3 'head' mIoU: 0.7508\n",
      "----------------------------------------------------------------\n",
      "Epoch [2/25], Training Loss: 0.0384\n",
      "Epoch [2/25], Validation the average of three class miou: 0.8166\n",
      " - Class 1 'turtle' mIoU: 0.8943\n",
      " - Class 2 'flipper' mIoU: 0.7716\n",
      " - Class 3 'head' mIoU: 0.7841\n",
      "----------------------------------------------------------------\n",
      "Epoch [3/25], Training Loss: 0.0232\n",
      "Epoch [3/25], Validation the average of three class miou: 0.8205\n",
      " - Class 1 'turtle' mIoU: 0.9047\n",
      " - Class 2 'flipper' mIoU: 0.7620\n",
      " - Class 3 'head' mIoU: 0.7948\n",
      "----------------------------------------------------------------\n",
      "Epoch [4/25], Training Loss: 0.0187\n",
      "Epoch [4/25], Validation the average of three class miou: 0.8000\n",
      " - Class 1 'turtle' mIoU: 0.8761\n",
      " - Class 2 'flipper' mIoU: 0.7694\n",
      " - Class 3 'head' mIoU: 0.7545\n",
      "----------------------------------------------------------------\n",
      "Epoch [5/25], Training Loss: 0.0175\n",
      "Epoch [5/25], Validation the average of three class miou: 0.7839\n",
      " - Class 1 'turtle' mIoU: 0.8229\n",
      " - Class 2 'flipper' mIoU: 0.7668\n",
      " - Class 3 'head' mIoU: 0.7620\n",
      "----------------------------------------------------------------\n",
      "Epoch [6/25], Training Loss: 0.0224\n",
      "Epoch [6/25], Validation the average of three class miou: 0.8415\n",
      " - Class 1 'turtle' mIoU: 0.9117\n",
      " - Class 2 'flipper' mIoU: 0.8076\n",
      " - Class 3 'head' mIoU: 0.8052\n",
      "----------------------------------------------------------------\n",
      "Epoch [7/25], Training Loss: 0.0132\n",
      "Epoch [7/25], Validation the average of three class miou: 0.8513\n",
      " - Class 1 'turtle' mIoU: 0.9176\n",
      " - Class 2 'flipper' mIoU: 0.8218\n",
      " - Class 3 'head' mIoU: 0.8144\n",
      "----------------------------------------------------------------\n",
      "Epoch [8/25], Training Loss: 0.0115\n",
      "Epoch [8/25], Validation the average of three class miou: 0.8625\n",
      " - Class 1 'turtle' mIoU: 0.9198\n",
      " - Class 2 'flipper' mIoU: 0.8297\n",
      " - Class 3 'head' mIoU: 0.8379\n",
      "----------------------------------------------------------------\n",
      "Epoch [9/25], Training Loss: 0.0104\n",
      "Epoch [9/25], Validation the average of three class miou: 0.8641\n",
      " - Class 1 'turtle' mIoU: 0.9261\n",
      " - Class 2 'flipper' mIoU: 0.8323\n",
      " - Class 3 'head' mIoU: 0.8340\n",
      "----------------------------------------------------------------\n",
      "Epoch [10/25], Training Loss: 0.0096\n",
      "Epoch [10/25], Validation the average of three class miou: 0.8632\n",
      " - Class 1 'turtle' mIoU: 0.9136\n",
      " - Class 2 'flipper' mIoU: 0.8351\n",
      " - Class 3 'head' mIoU: 0.8410\n",
      "----------------------------------------------------------------\n",
      "Epoch [11/25], Training Loss: 0.0089\n",
      "Epoch [11/25], Validation the average of three class miou: 0.8677\n",
      " - Class 1 'turtle' mIoU: 0.9242\n",
      " - Class 2 'flipper' mIoU: 0.8383\n",
      " - Class 3 'head' mIoU: 0.8407\n",
      "----------------------------------------------------------------\n",
      "Epoch [12/25], Training Loss: 0.0135\n",
      "Epoch [12/25], Validation the average of three class miou: 0.7196\n",
      " - Class 1 'turtle' mIoU: 0.8141\n",
      " - Class 2 'flipper' mIoU: 0.6996\n",
      " - Class 3 'head' mIoU: 0.6450\n",
      "----------------------------------------------------------------\n",
      "Epoch [13/25], Training Loss: 0.0167\n",
      "Epoch [13/25], Validation the average of three class miou: 0.8582\n",
      " - Class 1 'turtle' mIoU: 0.9206\n",
      " - Class 2 'flipper' mIoU: 0.8313\n",
      " - Class 3 'head' mIoU: 0.8225\n",
      "----------------------------------------------------------------\n",
      "Epoch [14/25], Training Loss: 0.0104\n",
      "Epoch [14/25], Validation the average of three class miou: 0.8672\n",
      " - Class 1 'turtle' mIoU: 0.9212\n",
      " - Class 2 'flipper' mIoU: 0.8369\n",
      " - Class 3 'head' mIoU: 0.8434\n",
      "----------------------------------------------------------------\n",
      "Epoch [15/25], Training Loss: 0.0085\n",
      "Epoch [15/25], Validation the average of three class miou: 0.8715\n",
      " - Class 1 'turtle' mIoU: 0.9278\n",
      " - Class 2 'flipper' mIoU: 0.8412\n",
      " - Class 3 'head' mIoU: 0.8455\n",
      "----------------------------------------------------------------\n",
      "Epoch [16/25], Training Loss: 0.0078\n",
      "Epoch [16/25], Validation the average of three class miou: 0.8735\n",
      " - Class 1 'turtle' mIoU: 0.9273\n",
      " - Class 2 'flipper' mIoU: 0.8418\n",
      " - Class 3 'head' mIoU: 0.8514\n",
      "----------------------------------------------------------------\n",
      "Epoch [17/25], Training Loss: 0.0074\n",
      "Epoch [17/25], Validation the average of three class miou: 0.8721\n",
      " - Class 1 'turtle' mIoU: 0.9267\n",
      " - Class 2 'flipper' mIoU: 0.8425\n",
      " - Class 3 'head' mIoU: 0.8472\n",
      "----------------------------------------------------------------\n",
      "Epoch [18/25], Training Loss: 0.0072\n",
      "Epoch [18/25], Validation the average of three class miou: 0.8714\n",
      " - Class 1 'turtle' mIoU: 0.9247\n",
      " - Class 2 'flipper' mIoU: 0.8440\n",
      " - Class 3 'head' mIoU: 0.8454\n",
      "----------------------------------------------------------------\n",
      "Epoch [19/25], Training Loss: 0.0070\n",
      "Epoch [19/25], Validation the average of three class miou: 0.8755\n",
      " - Class 1 'turtle' mIoU: 0.9272\n",
      " - Class 2 'flipper' mIoU: 0.8471\n",
      " - Class 3 'head' mIoU: 0.8521\n",
      "----------------------------------------------------------------\n",
      "Epoch [20/25], Training Loss: 0.0069\n",
      "Epoch [20/25], Validation the average of three class miou: 0.8742\n",
      " - Class 1 'turtle' mIoU: 0.9234\n",
      " - Class 2 'flipper' mIoU: 0.8419\n",
      " - Class 3 'head' mIoU: 0.8574\n",
      "----------------------------------------------------------------\n",
      "Epoch [21/25], Training Loss: 0.0067\n",
      "Epoch [21/25], Validation the average of three class miou: 0.8740\n",
      " - Class 1 'turtle' mIoU: 0.9251\n",
      " - Class 2 'flipper' mIoU: 0.8468\n",
      " - Class 3 'head' mIoU: 0.8501\n",
      "----------------------------------------------------------------\n",
      "Epoch [22/25], Training Loss: 0.0068\n",
      "Epoch [22/25], Validation the average of three class miou: 0.8624\n",
      " - Class 1 'turtle' mIoU: 0.9177\n",
      " - Class 2 'flipper' mIoU: 0.8320\n",
      " - Class 3 'head' mIoU: 0.8374\n",
      "----------------------------------------------------------------\n",
      "Epoch [23/25], Training Loss: 0.0234\n",
      "Epoch [23/25], Validation the average of three class miou: 0.8415\n",
      " - Class 1 'turtle' mIoU: 0.9026\n",
      " - Class 2 'flipper' mIoU: 0.8105\n",
      " - Class 3 'head' mIoU: 0.8112\n",
      "----------------------------------------------------------------\n",
      "Epoch [24/25], Training Loss: 0.0104\n",
      "Epoch [24/25], Validation the average of three class miou: 0.8704\n",
      " - Class 1 'turtle' mIoU: 0.9230\n",
      " - Class 2 'flipper' mIoU: 0.8439\n",
      " - Class 3 'head' mIoU: 0.8442\n",
      "----------------------------------------------------------------\n",
      "Epoch [25/25], Training Loss: 0.0079\n",
      "Epoch [25/25], Validation the average of three class miou: 0.8718\n",
      " - Class 1 'turtle' mIoU: 0.9217\n",
      " - Class 2 'flipper' mIoU: 0.8439\n",
      " - Class 3 'head' mIoU: 0.8496\n",
      "----------------------------------------------------------------\n",
      "Testing the average of three class miou: 0.8660\n",
      " - Class 1 'turtle' mIoU: 0.9278\n",
      " - Class 2 'flipper' mIoU: 0.8459\n",
      " - Class 3 'head' mIoU: 0.8241\n"
     ]
    }
   ],
   "source": [
    "# caculate iou function\n",
    "def calculate_iou(pred_mask, true_mask, num_classes):\n",
    "    ious = []\n",
    "    pred_mask = pred_mask.view(-1)  \n",
    "    true_mask = true_mask.view(-1)  \n",
    "\n",
    "    for cls in range(1, num_classes):  # only caculate iou for three classes\n",
    "        pred_inds = (pred_mask == cls)\n",
    "        true_inds = (true_mask == cls)\n",
    "        \n",
    "        intersection = (pred_inds & true_inds).sum().float()  \n",
    "        union = (pred_inds | true_inds).sum().float()  \n",
    "\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # avoid denominators of zero\n",
    "        else:\n",
    "            ious.append((intersection / union).item())  # append iou in list\n",
    "\n",
    "    return ious # return list\n",
    "\n",
    "# ----------------------\n",
    "# Training Phase\n",
    "# ----------------------\n",
    "num_epochs = 25 # set number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # train model\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in train_dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        masks = masks.squeeze(1)  \n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(images)['out']  \n",
    "        loss = criterion(outputs, masks.long())  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "        running_loss += loss.item()  # caculate loss\n",
    "\n",
    "    # print loss\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # ----------------------\n",
    "    # Validation Phase\n",
    "    # ----------------------\n",
    "    model.eval()  # eval model\n",
    "    all_ious = []  # record all iou in one epoch\n",
    "\n",
    "    with torch.no_grad():  # no grad\n",
    "        for images, masks in valid_dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            masks = masks.squeeze(1)\n",
    "\n",
    "            outputs = model(images)['out']\n",
    "            pred_masks = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                ious = calculate_iou(pred_masks[i], masks[i], num_classes=num_classes)\n",
    "                all_ious.append(ious)\n",
    "\n",
    "    # caculate miou and average of total miou\n",
    "    all_ious_tensor = torch.tensor(all_ious)\n",
    "    mean_ious = torch.nanmean(all_ious_tensor, dim=0) \n",
    "    avg_all_mious = torch.nanmean(mean_ious).item()\n",
    "\n",
    "    # print results\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation the average of three class miou: {avg_all_mious:.4f}\")\n",
    "    for cls in range(num_classes):\n",
    "        if cls == 0:\n",
    "            print(f\" - Class {cls + 1} 'turtle' mIoU: {mean_ious[cls].item():.4f}\")\n",
    "        elif cls == 1:\n",
    "            print(f\" - Class {cls + 1} 'flipper' mIoU: {mean_ious[cls].item():.4f}\")\n",
    "        elif cls == 2:\n",
    "            print(f\" - Class {cls + 1} 'head' mIoU: {mean_ious[cls].item():.4f}\")\n",
    "    \n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "# ----------------------\n",
    "# Testing Phase\n",
    "# ----------------------\n",
    "model.eval()\n",
    "test_ious = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        masks = masks.squeeze(1)\n",
    "        \n",
    "        # [betch size, number of classes, height, width] -> [betch size, number of classes, height, width] \n",
    "        outputs = model(images)['out'] \n",
    "        # [betch size, number of classes, height, width] -> [betch size, height, width] \n",
    "        pred_masks = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "         # image.size = [betch size, number of classes, height, width]\n",
    "         # image.size(0) = betch size = 16\n",
    "         # for i in range(16)\n",
    "        for i in range(images.size(0)):\n",
    "            ious = calculate_iou(pred_masks[i], masks[i], num_classes=num_classes)\n",
    "            test_ious.append(ious)\n",
    "\n",
    "# caculate miou and average of total miou\n",
    "all_ious_tensor = torch.tensor(test_ious)\n",
    "mean_ious = torch.nanmean(all_ious_tensor, dim=0)\n",
    "avg_all_mious = torch.nanmean(mean_ious).item()\n",
    "\n",
    "# print results\n",
    "print(f\"Testing the average of three class miou: {avg_all_mious:.4f}\")\n",
    "for cls in range(num_classes):\n",
    "    if cls == 0:\n",
    "        print(f\" - Class {cls + 1} 'turtle' mIoU: {mean_ious[cls].item():.4f}\")\n",
    "    elif cls == 1:\n",
    "        print(f\" - Class {cls + 1} 'flipper' mIoU: {mean_ious[cls].item():.4f}\")\n",
    "    elif cls == 2:\n",
    "        print(f\" - Class {cls + 1} 'head' mIoU: {mean_ious[cls].item():.4f}\")\n",
    "        \n",
    "        \n",
    "    #images [betch size, number of classes, height, width] \n",
    "    #masks [betch size, height, width]\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T16:23:02.585351Z",
     "start_time": "2024-11-03T16:23:02.577369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, 4):\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
